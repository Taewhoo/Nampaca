import os
import sys
import torch
import transformers
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast

#############################################
PROMPT_DICT = {
    "prompt_input": (
        "Below is an instruction that describes a task, paired with an input that provides further context.\n"
        "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\n\n"
        "Write a response that appropriately completes the request.\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "### Instruction(ëª…ë ¹ì–´):\n{instruction}\n\n### Input(ì…ë ¥):\n{input}\n\n### Response(ì‘ë‹µ):"
    ),
    "prompt_no_input": (
        "Below is an instruction that describes a task.\n"
        "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n\n"
        "Write a response that appropriately completes the request.\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "### Instruction(ëª…ë ¹ì–´):\n{instruction}\n\n### Response(ì‘ë‹µ):"
    ),
}

nampaca_koronly = "./finetuned"
nampaca_koreng  = "./finetuned2"

def gen(prompt, user_input=None, max_new_tokens=128, temperature=0.5):
    if user_input:
        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, input=user_input)
    else:
        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)
    
    input_ids = tokenizer.encode(x, return_tensors="pt").to('cuda:0')
    gen_tokens = model.generate(
        input_ids, 
        max_new_tokens=max_new_tokens, 
        num_return_sequences=1, 
        temperature=temperature,
        no_repeat_ngram_size=6,
        do_sample=True,
    )
    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)
    
    return gen_text.replace(x, '')
#############################################

#############################################
tokenizer = PreTrainedTokenizerFast.from_pretrained(nampaca_koreng)
# tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
model = GPT2LMHeadModel.from_pretrained(nampaca_koreng).to('cuda:0')
# model.generate(**tokenizer('ì•ˆë…•í•˜ì„¸ìš”?', return_tensors='pt').to('cuda:0'))
#############################################

# Example usage:
prompt = input("Instruction ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ")
put_input = input("Input ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ")
generated_text = gen(prompt, put_input)
print(f"\n\033[34mğŸŒ ğŸ¦™ â­ï¸ ë‚¨íŒŒì¹´ â­ï¸ ğŸ¦™ ğŸŒ:\n{generated_text}\033[0m")