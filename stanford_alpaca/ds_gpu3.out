bash: torchrun: command not found
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.45it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.42it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.41it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.43it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.44it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.42it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.41it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.43it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.43it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.41it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.43it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.41it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.42it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.42it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.41it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.42it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.42it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.41it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.41it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.42it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.41it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.43it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.41it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.41it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.41it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.42it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.42it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.42it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.41it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.42it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.43it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.42it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.42it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.43it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.42it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.42it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.43it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.41it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.41it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.41it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.24it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.25it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.25it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.26it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_160825-hr69b4l3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-snowflake-2
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/hr69b4l3
  0%|          | 0/1218 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1893, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2637, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2669, in compute_loss
    outputs = model(**inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 748, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 772, in forward
    outputs = self.model(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 621, in forward
    layer_outputs = decoder_layer(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 748, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 318, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 255, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/functional.py", line 1845, in softmax
    ret = input.softmax(dim, dtype=dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 31.75 GiB total capacity; 24.75 GiB already allocated; 31.88 MiB free; 25.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 🚀 View run misty-snowflake-2 at: https://wandb.ai/thlee/huggingface/runs/hr69b4l3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_160825-hr69b4l3/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30414 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30415 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30416 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 30413) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_16:08:45
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 30413)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.50it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.43it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.43it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.48it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.48it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.42it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.43it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.48it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.46it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.42it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.43it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.47it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.45it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.41it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.42it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.47it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.41it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.42it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.46it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.41it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.42it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.46it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.43it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.42it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.46it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.43it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.41it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.42it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.45it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.43it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.41it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.42it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.45it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.45it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.44it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.41it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.41it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.41it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.45it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.41it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.41it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.41it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.45it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.41it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.41it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.42it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.45it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.46it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.45it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.41it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.41it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.44it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.39it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.39it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.37it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.37it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.41it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.35it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.36it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:05,  2.36it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:05,  2.35it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.35it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.35it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.44it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.34it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.34it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.46it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.34it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.33it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.47it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.33it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.33it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.49it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:03,  2.33it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:03,  2.33it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.49it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.33it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.33it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.50it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.33it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.33it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.50it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.41it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.33it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.33it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.49it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.49it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.33it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.33it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.48it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:13<00:00,  2.33it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:13<00:00,  2.33it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.44it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.33it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.33it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.31it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.43it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.29it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.42it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.20it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.20it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.36it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.36it/s]

Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_161451-bb3123nd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sponge-3
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/bb3123nd
  0%|          | 0/1623 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1893, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2637, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2669, in compute_loss
    outputs = model(**inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 748, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 772, in forward
    outputs = self.model(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 621, in forward
    layer_outputs = decoder_layer(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 739, in forward
    args, kwargs = _pre_forward(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 413, in _pre_forward
    unshard_fn()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 434, in _pre_forward_unshard
    _unshard(state, handles, state._streams["unshard"], state._streams["pre_unshard"])
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 329, in _unshard
    handle.unshard()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 918, in unshard
    unsharded_flat_param = self._alloc_padded_unsharded_flat_param()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 944, in _alloc_padded_unsharded_flat_param
    _alloc_storage(unsharded_flat_param, flat_param._padded_unsharded_size)  # type: ignore[attr-defined]
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_utils.py", line 79, in _alloc_storage
    tensor._typed_storage()._resize_(size.numel())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/storage.py", line 764, in _resize_
    self._untyped_storage.resize_(size * self._element_size())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 31.75 GiB total capacity; 24.49 GiB already allocated; 341.88 MiB free; 24.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: 🚀 View run warm-sponge-3 at: https://wandb.ai/thlee/huggingface/runs/bb3123nd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_161451-bb3123nd/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21552 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21556 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21561 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 21547) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_16:15:10
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 21547)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.61it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.58it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.53it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.45it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.54it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.52it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.49it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.44it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.51it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.50it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.49it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.50it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.44it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.49it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.49it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:11,  2.49it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.43it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:11,  2.49it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.49it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.49it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.48it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.49it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.49it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.47it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.49it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.48it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.42it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.47it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.49it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.48it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.46it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.42it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.50it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.48it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.41it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.49it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.47it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.41it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.44it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.47it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.46it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.47it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.42it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.47it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.46it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.42it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.44it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.48it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.46it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.42it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.44it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.48it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.45it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.44it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.48it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.45it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.44it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.48it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.44it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.44it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.48it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.44it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.44it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.49it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.45it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.49it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.45it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.44it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.49it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.45it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.44it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.49it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.41it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.44it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.48it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.45it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.44it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.48it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.45it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.44it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.48it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.45it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.44it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.47it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.45it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.44it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.47it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.45it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.44it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.47it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.43it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.45it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.44it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.47it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.43it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.45it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.44it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.47it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.45it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.44it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.46it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.44it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.45it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.44it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.45it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.27it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.43it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.43it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.29it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.46it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.33it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.42it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_161904-vkyg21a8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-wood-4
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/vkyg21a8
  0%|          | 0/4875 [00:00<?, ?it/s]  0%|          | 1/4875 [00:10<13:32:37, 10.00s/it]                                                   {'loss': 1.6678, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/4875 [00:10<13:32:37, 10.00s/it]  0%|          | 2/4875 [00:20<13:38:57, 10.08s/it]                                                   {'loss': 1.9858, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 2/4875 [00:20<13:38:57, 10.08s/it]  0%|          | 3/4875 [00:30<13:43:05, 10.14s/it]                                                   {'loss': 1.7454, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 3/4875 [00:30<13:43:05, 10.14s/it]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 118, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 31.75 GiB total capacity; 25.05 GiB already allocated; 131.88 MiB free; 25.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁
wandb:   train/global_step ▁▅█
wandb: train/learning_rate ▁▁▁
wandb:          train/loss ▁█▃
wandb: 
wandb: Run summary:
wandb:         train/epoch 0.0
wandb:   train/global_step 3
wandb: train/learning_rate 0.0
wandb:          train/loss 1.7454
wandb: 
wandb: 🚀 View run ancient-wood-4 at: https://wandb.ai/thlee/huggingface/runs/vkyg21a8
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_161904-vkyg21a8/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27117 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27118 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27119 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27116) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_16:20:02
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27116)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.60it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.64it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.64it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.60it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.62it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.60it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.60it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.59it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.59it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.59it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.60it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.58it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:09,  2.60it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.59it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.59it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.59it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.59it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.58it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.59it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.59it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.56it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.56it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.57it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.57it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:07,  2.57it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:07,  2.57it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.58it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.58it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.59it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.59it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.60it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.59it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.59it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.59it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.59it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.59it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.59it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.59it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:05,  2.59it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:05,  2.59it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.59it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.59it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.59it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.59it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.59it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.59it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.57it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.57it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.55it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.55it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.56it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.55it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.56it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.56it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.57it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.57it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.57it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.57it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.58it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.58it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:11<00:00,  2.58it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.58it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.59it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.59it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.56it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.40it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.56it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_163952-k9i3tgjf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sun-5
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/k9i3tgjf
  0%|          | 0/4875 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1893, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2647, in training_step
    self.scaler.scale(loss).backward()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 31.75 GiB total capacity; 24.23 GiB already allocated; 223.88 MiB free; 25.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: 🚀 View run comfy-sun-5 at: https://wandb.ai/thlee/huggingface/runs/k9i3tgjf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_163952-k9i3tgjf/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 9836 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 9835) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_16:40:10
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 9835)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:08,  3.73it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:08,  3.76it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:08,  3.75it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:07,  3.75it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:07,  3.75it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:07,  3.76it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:06,  3.77it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:06,  3.77it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:06,  3.78it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:06,  3.77it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  3.77it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:03<00:05,  3.77it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:03<00:05,  3.77it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:05,  3.77it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:03<00:04,  3.77it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:04<00:04,  3.77it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:04<00:04,  3.77it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:04,  3.74it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:05<00:03,  3.76it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:05<00:03,  3.77it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:05<00:03,  3.77it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:05<00:02,  3.77it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:06<00:02,  3.77it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:06<00:02,  3.77it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:06<00:02,  3.77it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:06<00:01,  3.78it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:01,  3.77it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:07<00:01,  3.73it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:07<00:01,  3.75it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:07<00:00,  3.75it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:08<00:00,  3.77it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:08<00:00,  3.78it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.50it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.74it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1715, in _inner_training_loop
    model = self._wrap_model(self.model_wrapped)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1460, in _wrap_model
    self.model = model = FSDP(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 408, in __init__
    _init_param_handle_from_module(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 429, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 525, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 366, in __init__
    self._init_flat_param(params, fully_sharded_module, use_orig_params)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 462, in _init_flat_param
    self.flat_param = FlatParamHandle.flatten_params(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 505, in flatten_params
    flat_param_data = torch.cat(flat_params, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 31.75 GiB total capacity; 25.10 GiB already allocated; 551.88 MiB free; 25.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 21037) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:09:01
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 21037)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.66it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.61it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.65it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.60it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.62it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.59it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.61it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.60it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.61it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.60it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.61it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.61it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:09,  2.61it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:09,  2.61it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.61it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.61it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.61it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.61it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.60it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.60it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:07,  2.60it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:07,  2.60it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.60it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.60it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.61it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.61it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.61it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.61it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.61it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.61it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.61it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.61it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.61it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.61it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.60it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.61it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.61it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.61it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.61it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.61it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.60it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.60it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.60it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.60it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.61it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.61it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:09<00:02,  2.61it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:09<00:02,  2.61it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.61it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.61it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.61it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.60it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.60it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.61it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.61it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.61it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:11<00:00,  2.61it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:11<00:00,  2.61it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.61it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.61it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.59it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.45it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.59it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_171249-e5kxwwkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-fog-6
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/e5kxwwkb
  0%|          | 0/9750 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1893, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2647, in training_step
    self.scaler.scale(loss).backward()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 31.75 GiB total capacity; 24.52 GiB already allocated; 221.88 MiB free; 25.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: 🚀 View run expert-fog-6 at: https://wandb.ai/thlee/huggingface/runs/e5kxwwkb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_171249-e5kxwwkb/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 26460 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 26457) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:13:04
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 26457)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:08,  3.76it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:08,  3.77it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:07,  3.77it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:07,  3.77it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:07,  3.77it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:07,  3.76it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:06,  3.76it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:06,  3.75it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:06,  3.76it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:06,  3.77it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  3.73it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:03<00:05,  3.74it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:03<00:05,  3.75it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:05,  3.73it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:03<00:04,  3.74it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:04<00:04,  3.75it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:04<00:04,  3.76it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:03,  3.77it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:05<00:03,  3.74it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:05<00:03,  3.75it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:05<00:03,  3.76it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:05<00:02,  3.76it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:06<00:02,  3.73it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:06<00:02,  3.73it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:06<00:02,  3.75it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:06<00:01,  3.71it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:01,  3.73it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:07<00:01,  3.74it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:07<00:01,  3.75it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:07<00:00,  3.76it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:08<00:00,  3.76it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:08<00:00,  3.77it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.50it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.72it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1715, in _inner_training_loop
    model = self._wrap_model(self.model_wrapped)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1460, in _wrap_model
    self.model = model = FSDP(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 408, in __init__
    _init_param_handle_from_module(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 429, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 525, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 366, in __init__
    self._init_flat_param(params, fully_sharded_module, use_orig_params)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 462, in _init_flat_param
    self.flat_param = FlatParamHandle.flatten_params(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 505, in flatten_params
    flat_param_data = torch.cat(flat_params, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 31.75 GiB total capacity; 25.10 GiB already allocated; 551.88 MiB free; 25.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 31486) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:27:24
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31486)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:08,  3.70it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:08,  3.70it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:08,  3.71it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:07,  3.71it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:07,  3.71it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:07,  3.67it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:07,  3.69it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:06,  3.70it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:06,  3.70it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:06,  3.70it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  3.71it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:03<00:05,  3.71it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:03<00:05,  3.71it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:05,  3.71it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:04<00:04,  3.70it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:04<00:04,  3.70it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:04<00:04,  3.70it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:04,  3.67it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:05<00:03,  3.68it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:05<00:03,  3.69it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:05<00:03,  3.70it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:05<00:02,  3.70it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:06<00:02,  3.71it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:06<00:02,  3.71it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:06<00:02,  3.71it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:07<00:01,  3.71it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:01,  3.71it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:07<00:01,  3.71it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:07<00:01,  3.71it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:08<00:00,  3.69it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:08<00:00,  3.69it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:08<00:00,  3.70it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.41it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.67it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1715, in _inner_training_loop
    model = self._wrap_model(self.model_wrapped)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1460, in _wrap_model
    self.model = model = FSDP(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 408, in __init__
    _init_param_handle_from_module(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 429, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 525, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 366, in __init__
    self._init_flat_param(params, fully_sharded_module, use_orig_params)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 462, in _init_flat_param
    self.flat_param = FlatParamHandle.flatten_params(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/flat_param.py", line 505, in flatten_params
    flat_param_data = torch.cat(flat_params, dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 31.75 GiB total capacity; 25.10 GiB already allocated; 551.88 MiB free; 25.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 32854) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:31:01
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32854)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:11,  2.84it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:10,  2.85it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:10,  2.80it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.61it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:10,  2.76it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.64it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.73it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.64it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:09,  2.72it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.63it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:09,  2.71it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.62it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:09,  2.70it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.62it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:08,  2.70it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.67it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.59it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.68it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.59it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:07,  2.68it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.59it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:07,  2.68it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.60it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.69it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.69it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.60it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:05<00:06,  2.68it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:07,  2.61it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.66it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.61it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.66it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.61it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.65it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.61it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.64it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.62it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:07<00:04,  2.64it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.62it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.63it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.62it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.62it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.61it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:08<00:03,  2.62it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.62it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.63it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.62it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:09<00:02,  2.63it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.62it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.62it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.62it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.62it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.62it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:10<00:01,  2.63it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:09<00:02,  2.62it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.62it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.62it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:11<00:00,  2.63it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.62it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.63it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.62it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.62it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.46it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.65it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards:  94%|█████████▍| 31/33 [00:11<00:00,  2.66it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.48it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.60it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_174634-lonmcnx6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-breeze-7
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/lonmcnx6
  0%|          | 0/78003 [00:00<?, ?it/s]  0%|          | 1/78003 [00:03<77:52:56,  3.59s/it]                                                    {'loss': 0.9108, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/78003 [00:03<77:52:56,  3.59s/it]  0%|          | 2/78003 [00:06<65:40:58,  3.03s/it]                                                    {'loss': 2.186, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 2/78003 [00:06<65:40:58,  3.03s/it]  0%|          | 3/78003 [00:08<61:54:00,  2.86s/it]                                                    {'loss': 1.5364, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 3/78003 [00:08<61:54:00,  2.86s/it]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 114, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 31.75 GiB total capacity; 29.86 GiB already allocated; 338.88 MiB free; 30.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 118, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 1; 31.75 GiB total capacity; 30.24 GiB already allocated; 164.88 MiB free; 30.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 38059 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 38060) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:46:51
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 38060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁
wandb:   train/global_step ▁▅█
wandb: train/learning_rate ▁▁▁
wandb:          train/loss ▁█▄
wandb: 
wandb: Run summary:
wandb:         train/epoch 0.0
wandb:   train/global_step 3
wandb: train/learning_rate 0.0
wandb:          train/loss 1.5364
wandb: 
wandb: 🚀 View run smooth-breeze-7 at: https://wandb.ai/thlee/huggingface/runs/lonmcnx6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_174634-lonmcnx6/logs
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:08,  3.69it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:08,  3.69it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:08,  3.64it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:07,  3.66it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:07,  3.67it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:07,  3.68it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:07,  3.67it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:06,  3.67it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:06,  3.68it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:06,  3.68it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  3.69it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:03<00:05,  3.69it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:03<00:05,  3.70it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:05,  3.69it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:04<00:04,  3.67it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:04<00:04,  3.66it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:04<00:04,  3.66it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:04,  3.67it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:05<00:03,  3.67it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:05<00:03,  3.67it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:05<00:03,  3.67it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:05<00:02,  3.67it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:06<00:02,  3.68it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:06<00:02,  3.68it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:06<00:02,  3.68it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:07<00:01,  3.68it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:01,  3.67it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:07<00:01,  3.67it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:07<00:01,  3.67it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:08<00:00,  3.67it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:08<00:00,  3.67it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:08<00:00,  3.67it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.41it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.65it/s]
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_175100-av1xfmze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-valley-8
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/av1xfmze
  0%|          | 0/156006 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 2647, in training_step
    self.scaler.scale(loss).backward()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 31.75 GiB total capacity; 30.38 GiB already allocated; 294.88 MiB free; 30.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: 🚀 View run noble-valley-8 at: https://wandb.ai/thlee/huggingface/runs/av1xfmze
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_175100-av1xfmze/logs
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 39763) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:51:14
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 39763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py:1353: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.65it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.54it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.56it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:11,  2.59it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.53it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.56it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.56it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.52it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.56it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.55it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.51it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.55it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:11,  2.54it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:11,  2.51it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.55it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.54it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.51it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.55it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.54it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.52it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.55it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.54it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.52it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.55it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.54it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.52it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.55it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:09,  2.54it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:09,  2.52it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:09,  2.55it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.54it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.52it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:08,  2.55it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.54it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.53it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.56it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:07,  2.54it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:07,  2.53it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:07,  2.56it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.54it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.53it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.56it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:07,  2.54it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:07,  2.54it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:07,  2.56it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.54it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.54it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.56it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.55it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.53it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.54it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:05,  2.53it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:05,  2.54it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.54it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.54it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.54it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.54it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.54it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.55it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:03,  2.55it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.55it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.55it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.54it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.54it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.54it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.54it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.54it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.55it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:02,  2.55it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:01,  2.55it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:01,  2.55it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:10<00:01,  2.55it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.55it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.54it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.54it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.54it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.53it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.54it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.54it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.37it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.53it/s]
Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.52it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.53it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_175635-fnbhmn9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-breeze-9
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/fnbhmn9c
  0%|          | 0/52002 [00:00<?, ?it/s]  0%|          | 1/52002 [00:03<46:51:23,  3.24s/it]                                                    {'loss': 1.1598, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/52002 [00:03<46:51:23,  3.24s/it]  0%|          | 2/52002 [00:04<34:03:45,  2.36s/it]                                                    {'loss': 1.929, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 2/52002 [00:04<34:03:45,  2.36s/it]Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 118, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 1; 31.75 GiB total capacity; 30.30 GiB already allocated; 210.88 MiB free; 30.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 114, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 31.75 GiB total capacity; 30.04 GiB already allocated; 188.88 MiB free; 30.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1957, in _inner_training_loop
    self.scaler.step(self.optimizer)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/fsdp/sharded_grad_scaler.py", line 290, in step
    return super().step(optimizer, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 290, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 160, in step
    self._init_group(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/optim/adamw.py", line 118, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 2; 31.75 GiB total capacity; 30.30 GiB already allocated; 48.88 MiB free; 30.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1095 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 1096) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-03_17:56:52
  host      : mint
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1097)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_17:56:52
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1096)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁
wandb:   train/global_step ▁█
wandb: train/learning_rate ▁▁
wandb:          train/loss ▁█
wandb: 
wandb: Run summary:
wandb:         train/epoch 0.0
wandb:   train/global_step 2
wandb: train/learning_rate 0.0
wandb:          train/loss 1.929
wandb: 
wandb: 🚀 View run vague-breeze-9 at: https://wandb.ai/thlee/huggingface/runs/fnbhmn9c
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230403_175635-fnbhmn9c/logs
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-04-03 18:06:01,674] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1406, in __post_init__
    raise ValueError("--deepspeed requires Accelerate to be installed: `pip install accelerate`.")
ValueError: --deepspeed requires Accelerate to be installed: `pip install accelerate`.
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1406, in __post_init__
    raise ValueError("--deepspeed requires Accelerate to be installed: `pip install accelerate`.")
ValueError: --deepspeed requires Accelerate to be installed: `pip install accelerate`.
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1406, in __post_init__
    raise ValueError("--deepspeed requires Accelerate to be installed: `pip install accelerate`.")
ValueError: --deepspeed requires Accelerate to be installed: `pip install accelerate`.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7413) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-03_18:06:04
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 7415)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-04-03_18:06:04
  host      : mint
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 7418)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_18:06:04
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 7413)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-04-03 18:06:30,170] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1411, in __post_init__
    self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 77, in __init__
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 67, in __init__
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 52, in __init__
    config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode("utf-8")
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 133, in urlsafe_b64decode
    return b64decode(s)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 87, in b64decode
    return binascii.a2b_base64(s)
binascii.Error: Invalid base64-encoded string: number of data characters (13) cannot be 1 more than a multiple of 4
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1411, in __post_init__
    self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 77, in __init__
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 67, in __init__
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 52, in __init__
        config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode("utf-8")train()

  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 133, in urlsafe_b64decode
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 194, in train
    return b64decode(s)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 87, in b64decode
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()return binascii.a2b_base64(s)

  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/hf_argparser.py", line 332, in parse_args_into_dataclasses
binascii.Error: Invalid base64-encoded string: number of data characters (13) cannot be 1 more than a multiple of 4
    obj = dtype(**inputs)
  File "<string>", line 111, in __init__
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/training_args.py", line 1411, in __post_init__
    self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 77, in __init__
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 67, in __init__
    super().__init__(config_file_or_dict)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 52, in __init__
    config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode("utf-8")
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 133, in urlsafe_b64decode
    return b64decode(s)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/base64.py", line 87, in b64decode
    return binascii.a2b_base64(s)
binascii.Error: Invalid base64-encoded string: number of data characters (13) cannot be 1 more than a multiple of 4
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 8879) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-03_18:06:32
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 8880)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-04-03_18:06:32
  host      : mint
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 8898)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_18:06:32
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 8879)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-04-03 18:09:51,097] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-04-03 18:09:56,850] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.32it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.45it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.43it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.45it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.39it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.44it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.41it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.44it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.44it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.42it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.43it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:11,  2.43it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.43it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.44it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.43it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.44it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.44it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.44it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.43it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.44it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.44it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.43it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.43it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.44it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.43it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.43it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.42it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.43it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.43it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.43it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.43it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.43it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.43it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.43it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.43it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.43it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:06,  2.43it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.43it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.43it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.43it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.43it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.43it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.43it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.44it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.44it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.44it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.43it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.43it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.43it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.43it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.43it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.43it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:01,  2.43it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.43it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.43it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.43it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.43it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.44it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.43it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.43it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.29it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.41it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.42it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.42it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()Traceback (most recent call last):

  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
      File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
engine = DeepSpeedEngine(args=args,
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
        self._configure_optimizer(optimizer, model_parameters)deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)

  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
    raise ZeRORuntimeException(msg)
deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
    raise ZeRORuntimeException(msg)
deepspeed.runtime.zero.utils.ZeRORuntimeException:     You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)

  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
    raise ZeRORuntimeException(msg)
deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10851 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10849) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-03_18:11:38
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 10850)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_18:11:38
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10849)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-04-03 18:15:04,153] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-04-03 18:15:09,869] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.44it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.34it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.44it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.42it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.42it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.38it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.41it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.38it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.39it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.39it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.39it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.39it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.39it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.39it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.40it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.41it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.41it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.41it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.41it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.43it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.42it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.43it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:09<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.41it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.42it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.42it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.42it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.27it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.27it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.40it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
Traceback (most recent call last):
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 231, in <module>
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    train()
  File "/mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/train.py", line 225, in train
    trainer.train()
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1628, in train
    return inner_training_loop(
      File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
    return inner_training_loop(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/trainer.py", line 1697, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
      File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
      File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
self._configure_optimizer(optimizer, model_parameters)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
    engine = DeepSpeedEngine(args=args,
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)    
self._configure_optimizer(optimizer, model_parameters)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/__init__.py", line 125, in initialize
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
        engine = DeepSpeedEngine(args=args,raise ZeRORuntimeException(msg)

  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 340, in __init__
deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1281, in _configure_optimizer
    raise ZeRORuntimeException(msg)
deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
    raise ZeRORuntimeException(msg)
deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 12835) of binary: /home/namz2/.conda/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/home/namz2/.conda/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-03_18:16:56
  host      : mint
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 12836)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-04-03_18:16:56
  host      : mint
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 12837)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-03_18:16:56
  host      : mint
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 12835)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-04-03 18:24:21,121] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-04-03 18:24:26,625] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.41it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.35it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.40it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.37it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:12,  2.39it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.38it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.38it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.39it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.40it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.40it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.39it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.40it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.39it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.39it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.40it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.40it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.40it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.40it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:07,  2.40it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.40it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.39it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.40it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.40it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.40it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:06<00:07,  2.40it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.40it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.40it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.40it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:07<00:05,  2.40it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.40it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.40it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.40it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.40it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.40it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:08<00:04,  2.40it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:09<00:04,  2.40it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.40it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.40it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.40it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:10<00:02,  2.40it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.40it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.40it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  2.41it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:12<00:01,  2.41it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.41it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.41it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.41it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.41it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.27it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.38it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.25it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.39it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.25it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.39it/s]
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/namz2/.cache/torch_extensions/py39_cu117/utils...
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/namz2/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/include -isystem /home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/include/TH -isystem /home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/include/THC -isystem /home/namz2/.conda/envs/alpaca/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o 
[2/2] c++ flatten_unflatten.o -shared -L/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so
Loading extension module utils...
Time to load utils op: 18.57896614074707 seconds
Loading extension module utils...
Time to load utils op: 18.430625438690186 seconds
Loading extension module utils...
Time to load utils op: 18.631016969680786 seconds
Parameter Offload: Total persistent parameters: 0 in 0 params
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...

Loading extension module utils...Loading extension module utils...

Time to load utils op: 0.0006623268127441406 secondsTime to load utils op: 0.0006573200225830078 seconds

/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Using /home/namz2/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003638267517089844 seconds
wandb: Currently logged in as: dlxogn12345 (thlee). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /mnt/data1/namz2/taewhoo/NLP/stanford_alpaca/wandb/run-20230403_182739-bkrix9xf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sea-10
wandb: ⭐️ View project at https://wandb.ai/thlee/huggingface
wandb: 🚀 View run at https://wandb.ai/thlee/huggingface/runs/bkrix9xf
  0%|          | 0/3465 [00:00<?, ?it/s]/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/home/namz2/.conda/envs/alpaca/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
  0%|          | 1/3465 [00:24<23:28:50, 24.40s/it]                                                   {'loss': 1.3245, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/3465 [00:24<23:28:50, 24.40s/it]  0%|          | 2/3465 [00:48<23:02:11, 23.95s/it]                                                   {'loss': 1.5548, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 2/3465 [00:48<23:02:11, 23.95s/it]  0%|          | 3/3465 [01:11<22:45:24, 23.66s/it]                                                   {'loss': 1.7143, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 3/3465 [01:11<22:45:24, 23.66s/it]  0%|          | 4/3465 [01:34<22:39:03, 23.56s/it]                                                   {'loss': 1.5486, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 4/3465 [01:34<22:39:03, 23.56s/it]  0%|          | 5/3465 [01:58<22:33:57, 23.48s/it]                                                   {'loss': 1.5048, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 5/3465 [01:58<22:33:57, 23.48s/it]  0%|          | 6/3465 [02:21<22:33:08, 23.47s/it]                                                   {'loss': 1.6479, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 6/3465 [02:21<22:33:08, 23.47s/it]  0%|          | 7/3465 [02:44<22:27:10, 23.38s/it]                                                   {'loss': 1.4411, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 7/3465 [02:44<22:27:10, 23.38s/it]  0%|          | 8/3465 [03:07<22:23:32, 23.32s/it]                                                   {'loss': 1.4296, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 8/3465 [03:07<22:23:32, 23.32s/it]  0%|          | 9/3465 [03:31<22:24:03, 23.33s/it]                                                   {'loss': 1.3889, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 9/3465 [03:31<22:24:03, 23.33s/it]  0%|          | 10/3465 [03:54<22:22:35, 23.32s/it]                                                    {'loss': 1.4801, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 10/3465 [03:54<22:22:35, 23.32s/it]  0%|          | 11/3465 [04:17<22:20:22, 23.28s/it]                                                    {'loss': 1.5181, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 11/3465 [04:17<22:20:22, 23.28s/it]  0%|          | 12/3465 [04:41<22:22:48, 23.33s/it]                                                    {'loss': 1.6451, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 12/3465 [04:41<22:22:48, 23.33s/it]  0%|          | 13/3465 [05:04<22:21:00, 23.31s/it]                                                    {'loss': 1.6868, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 13/3465 [05:04<22:21:00, 23.31s/it]  0%|          | 14/3465 [05:27<22:20:30, 23.31s/it]                                                    {'loss': 1.7252, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 14/3465 [05:27<22:20:30, 23.31s/it]